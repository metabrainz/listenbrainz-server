MAILTO=""

# NOTE: This file is organized in chronological order of the start hour of each job, keeping jobs spread over the night and not overlapping.

# Delete pending listens and update our listen counts, timestamps hourly
0 * * * * root /usr/local/bin/python /code/listenbrainz/manage.py delete_listens_and_update_metadata >> /logs/listen_metadata.log 2>&1

# Generating troi daily playlists runs hourly
0 * * * * root /usr/local/bin/python /code/listenbrainz/manage.py run_daily_jams >> /logs/troi.log 2>&1

## Trigger an incremental dump everyday, near noon, far away from whole dump times, again do not block for the lock.
0 0 * * * root flock -x -n /var/lock/lb-dumps.lock /code/listenbrainz/admin/create-dumps.sh incremental >> /logs/dumps.log 2>&1
## Around 1 hour later, trigger an incremental import into the spark cluster, blocking for the lock in case the dump was not complete
0 1 * * * root flock -x /var/lock/lb-dumps.lock /usr/local/bin/python /code/listenbrainz/manage.py spark request_import_incremental >> /logs/dumps.log 2>&1

# After the daily dumping is done, make sure everything turned out ok, otherwise mail the observability list.
0 2 * * * root flock -x /var/lock/lb-dumps.lock /usr/local/bin/python /code/listenbrainz/manage.py dump check_dump_ages >> /logs/dumps.log 2>&1

# Dump user feedback before we generate recommendations
15 2 * * * root flock -x -n /var/lock/lb-dumps.lock /code/listenbrainz/admin/create-dumps.sh feedback >> /logs/dumps.log 2>&1

# batch up all the spark requests, spaced by 1 minute in order to ensure the correct order
# Request all statistics
0 2 * * * root /usr/local/bin/python /code/listenbrainz/manage.py spark cron_request_all_stats >> /logs/stats.log 2>&1
# Request recommendations after dump and mapping has been imported into the spark cluster
1 2 * * * root /usr/local/bin/python /code/listenbrainz/manage.py spark cron_request_recommendations >> /logs/stats.log 2>&1
# Request missing MB data each month after a new dump has been imported to spark cluster
2 2 * * 1 root /usr/local/bin/python /code/listenbrainz/manage.py spark request_missing_mb_data >> /logs/stats.log 2>&1
# Calculate user similarity
3 2 * * * root /usr/local/bin/python /code/listenbrainz/manage.py spark cron_request_similar_users >> /logs/stats.log 2>&1

## Trigger a full dump on 1st and 15th of every month, in the middle of the night, do not block to wait for lock.
0 4 1,15 * * root flock -x -n /var/lock/lb-dumps.lock /code/listenbrainz/admin/create-dumps.sh full >> /logs/dumps.log 2>&1
## Around 24 hours later, trigger a full import into the spark cluster, and this time wait for the lock, in case the dump hasn't finished.
0 4 2,16 * * root flock -x /var/lock/lb-dumps.lock /usr/local/bin/python /code/listenbrainz/manage.py spark request_import_full >> /logs/dumps.log 2>&1
