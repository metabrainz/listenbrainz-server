version: "3.4"

# IMPORTANT NOTE: Volume paths mounted on containers are relative to the
# directory that this file is in (`docker/`) and so probably need to start with
# `../` to refer to a directory in the main code checkout

volumes:
  spark-master:
  namenode:
  spark-worker:
  datanode:

services:
  spark-master:
    build:
      context: ..
      dockerfile: Dockerfile.spark
      target: metabrainz-spark-master
    command: sleep 1000000000
    volumes:
      - spark-master:/home/hadoop/spark:z

  hadoop-master:
    image: metabrainz/hadoop-yarn:beta
    command: hdfs namenode
    volumes:
      - namenode:/home/hadoop/hdfs:z

  spark-worker:
    build:
      context: ..
      dockerfile: Dockerfile.spark
      target: metabrainz-spark-master
    command: sleep 1000000000
    volumes:
      - spark-worker:/home/hadoop/spark:z

  datanode:
    image: metabrainz/hadoop-yarn:beta
    command: hdfs datanode
    volumes:
      - datanode:/home/hadoop/hdfs:z

  playground:
    build:
      context: ..
      dockerfile: Dockerfile.spark
      target: metabrainz-spark-dev
    command: sleep 1000000000
    volumes:
      - ..:/rec:z

  request_consumer:
    build:
      context: ..
      dockerfile: Dockerfile.spark
      target: metabrainz-spark-dev
    command: bash -c "zip -r listenbrainz_spark.zip listenbrainz_spark/ ; python spark_manage.py request_consumer"
    volumes:
      - ..:/rec:z

networks:
  default:
    external:
      name: listenbrainz_default
